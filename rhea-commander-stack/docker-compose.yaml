# ═══════════════════════════════════════════════════════════════
# Rhea Commander Stack v2 — Production Docker Compose
# LiteLLM (gateway) + LobeChat (UI) + ComfyUI (visual gen)
# Deploy: ./deploy.sh up
# Reverse: ./deploy.sh down  (or ./deploy.sh nuke for full wipe)
# ═══════════════════════════════════════════════════════════════

networks:
  rhea-net:
    driver: bridge

volumes:
  comfyui-data:
  litellm-logs:

services:

  # ─── Layer 1: LiteLLM Proxy (AI Gateway) ───
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: rhea-litellm
    ports:
      - "${LITELLM_PORT:-4000}:4000"
    volumes:
      - ./litellm_config.yaml:/app/config.yaml:ro
      - litellm-logs:/app/logs
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    env_file:
      - .env
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-rhea-commander-2026}
    networks:
      - rhea-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:4000/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s

  # ─── Layer 2: LobeChat (Operator Interface) ───
  lobechat:
    image: lobehub/lobe-chat:latest
    container_name: rhea-lobechat
    ports:
      - "${LOBECHAT_PORT:-3210}:3210"
    environment:
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY:-sk-rhea-commander-2026}
      - OPENAI_PROXY_URL=http://litellm:4000/v1
      - ACCESS_CODE=${LOBECHAT_ACCESS_CODE:-rhea2026}
      - ENABLED_OPENAI=1
      - CUSTOM_MODELS=claude-opus,claude-sonnet,gpt-4o,o3-mini,gemini-flash,gemini-pro,deepseek-v3,deepseek-r1,openrouter-auto,hf-inference
    networks:
      - rhea-net
    depends_on:
      litellm:
        condition: service_healthy
    restart: unless-stopped

  # ─── Layer 3: ComfyUI (Visual Generation — CPU mode) ───
  comfyui:
    image: ghcr.io/ai-dock/comfyui:pytorch-2.4.1-cpu
    container_name: rhea-comfyui
    ports:
      - "${COMFYUI_PORT:-8188}:8188"
    volumes:
      - comfyui-data:/workspace
    environment:
      - COMFYUI_FLAGS=--cpu --listen 0.0.0.0
    networks:
      - rhea-net
    restart: unless-stopped
    # ComfyUI CPU image is large (~5GB), first pull takes time
    # GPU users: swap image to ghcr.io/ai-dock/comfyui:pytorch-2.4.1-cuda-12.4.1
