Implement the following plan:

# Plan: Integrations Audit — Full Inventory with Liveness Testing

## Context

The AUTONOMY_WITH_AUDIT_ROOT.md (Section 6) mandates a first-class Integrations Audit as a Phase 1 deliverable. The goal is to prevent tool sprawl by creating a single registry of every tool, connector, and integration available to Rhea — with live pass/fail status for each one.

The user has confirmed:
- **Scope:** Full inventory across all 4 layers (plugins, MCP, bridge, scripts)
- **Testing:** Liveness-check every integration and record actual pass/fail
- **Broken integrations:** Document as broken, don't try to fix now

## Deliverable

**File:** `docs/INTEGRATIONS_AUDIT.md`

## Design: Flat Registry (Single File, 4 Sections)

### Document Structure

```
# Integrations Audit
> Generated: YYYY-MM-DD | Last liveness check: YYYY-MM-DD

## Summary
- Total integrations: N
- Passing: N | Failing: N | Untested: N

## Layer 1: Claude Code Plugins & Skills
(table)

## Layer 2: MCP Servers (claude.ai Connectors)
(table)

## Layer 3: rhea_bridge.py API Providers
(table)

## Layer 4: Local Scripts & CLI Tools
(table)

## Layer 5: Hooks & Lifecycle Integration
(table)

## Known Issues & Gaps
(list)
```

### Table Schema (per row)

| Column | Description |
|--------|-------------|
| Name | Tool/integration identifier |
| Capability | What it does (1 line) |
| Scope | What it can access/modify |
| Approval | none / user-confirm / tribunal |
| Audit Log | Where actions are logged |
| Failure Modes | Known ways it can fail |
| Test Command | How to verify it works |
| Status | PASS / FAIL / UNTESTED |

## Execution Steps

### Step 1: Catalog Layer 1 — Claude Code Plugins & Skills
- Read installed plugins from skills list in system context
- Known installed: github, feature-dev, superpowers, frontend-design, code-review, code-simplifier, commit-commands, playwright, pr-review-toolkit, figma, claude-code-setup, claude-md-management, linear, swift-lsp, firebase, huggingface-skills, firecrawl, coderabbit
- For each: document capability, scope, approval level
- Test: invoke a read-only operation where possible

### Step 2: Catalog Layer 2 — MCP Servers
- Enumerate from deferred tools list (ToolSearch)
- Known servers: ICD-10 Codes, Clinical Trials, Open Targets, Mermaid Chart, Asana, Linear, LunarCrush, Windsor.ai, Amplitude, bioRxiv, ChEMBL, Gamma, Synapse.org, BioRender, Learning Commons, Slack, Hugging Face, Clay, Scholar Gateway, Vibe Prospecting, Figma, Ahrefs, Consensus, Coupler.io, iMessages (from settings.local.json)
- For each: document capability, test with a simple read-only call
- Record PASS/FAIL with error message if failed

### Step 3: Catalog Layer 3 — rhea_bridge.py Providers
- Source: `src/rhea_bridge.py` PROVIDERS dict and MODEL_TIERS dict
- 6 providers: OpenAI, Gemini, DeepSeek, OpenRouter, HuggingFace, Azure
- 4 tiers: cheap, balanced, expensive, reasoning
- Test: `python3 src/rhea_bridge.py status` to check key availability
- Test: `python3 src/rhea_bridge.py tiers` to check tier availability
- Record which providers have keys set and which don't

### Step 4: Catalog Layer 4 — Local Scripts & CLI
- Source: `scripts/rhea/` directory, root-level scripts
- Known: `rhea` CLI (bootstrap, check, memory), `rhea_autosave.sh`, `rhea_watch.sh`, `rhea_commit.sh`, `entire_commit.sh`, `memory_benchmark.sh`
- Sub-modules: `scripts/rhea/lib_entire.sh`, `scripts/rhea/import_nested.sh`, `scripts/rhea/memory.sh`, `scripts/rhea/check.sh`
- Test: dry-run or `--help` for each script
- Record which ones are executable and functional

### Step 5: Catalog Layer 5 — Hooks & Lifecycle
- Source: `.claude/settings.json` hooks config
- Hooks: SessionStart, SessionEnd, UserPromptSubmit, Stop, PreToolUse(Task), PostToolUse(Task), PostToolUse(TodoWrite)
- All route through `entire hooks claude-code <event>`
- Test: confirm `entire` CLI is available and hooks are functional

### Step 6: Compile & Write
- Assemble all findings into `docs/INTEGRATIONS_AUDIT.md`
- Calculate summary stats
- List known issues and gaps

### Step 7: Verify
- Confirm file is well-formed markdown
- Confirm all 4+ layers are covered
- Confirm liveness status is populated for every row
- Check file fits the ROOT prompt's deliverable spec

## Critical Files

| File | Action |
|------|--------|
| `docs/INTEGRATIONS_AUDIT.md` | CREATE — the main deliverable |
| `src/rhea_bridge.py` | READ — extract provider/model/tier data |
| `.claude/settings.json` | READ — hooks and plugin config |
| `.claude/settings.local.json` | READ — local permissions and MCP tools |
| `scripts/rhea/` | READ — enumerate local scripts |
| `docs/CORE_RULES.md` | READ — verify alignment with Section 8 artifacts |

## Verification

1. `docs/INTEGRATIONS_AUDIT.md` exists and has all 5 layers
2. Every row has a Status column with PASS, FAIL, or UNTESTED
3. Summary section has accurate totals
4. File matches ROOT prompt Section 6 deliverable spec (tool name → capability → scope → approval needed → audit log location → failure modes → test command)
5. Known broken integrations (Fireflies, PubMed) are documented as FAIL


If you need specific details from before exiting plan mode (like exact code snippets, error messages, or content you generated), read the full transcript at: /Users/sa/.claude/projects/-Users-sa-rh-1/5e8ca6db-c55e-4d6d-aa15-4bc9fb29a388.jsonl

If this plan can be broken down into multiple independent tasks, consider using the TeamCreate tool to create a team and parallelize the work.