# Why Now

## 2026 is the first year this is buildable

Three things converged:

**1. Multi-model AI is commodity infrastructure.**
Six months ago, getting diverse AI perspectives required enterprise contracts with each provider. Today, OpenRouter alone gives access to 400+ models. DeepSeek and Gemini offer flagship reasoning at 10-100x lower cost than GPT-4-class models did in 2024. A single Python file (`rhea_bridge.py`, 900 lines) replaces what would have been a platform team.

**2. On-device ML reached the threshold.**
CoreML on Apple Silicon runs transformer inference fast enough for real-time circadian prediction. HealthKit exposes HRV, sleep stages, and activity at the resolution needed for passive profiling. The hardware for an offline-first health advisory system exists in every recent iPhone and Apple Watch.

**3. The science matured.**
- Polyvagal theory (Porges) gave us the autonomic framework: environment → nervous system state → behavior
- HRV research (Längle et al. 2025, Takeda et al. 2025) confirmed the proxy: vagal tone predicts cognitive control and ADHD severity
- Interoception studies (Bruton et al. 2025) explained why self-report fails for ADHD: diminished body-signal reading
- Cross-cultural analysis (Wiessner 2014, Yetish et al. 2015) revealed the pattern: elite rituals across civilizations independently reconstruct forager defaults

Each piece existed before. The combination — cheap multi-model AI + on-device inference + mature circadian science + passive biometric APIs — is new.

## Why not later

Every month of delay is another month of people using willpower-based tools that don't work for them. The infrastructure exists. The science is published. The gap is a system that connects them. That's what Rhea is.
